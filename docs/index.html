<!DOCTYPE html>

<html>

<head>
    <style>
        td,
        th {
            border: 0px solid black;
        }

        img {
            padding: 5px;
        }
    </style>

    <title>SegFace</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="shortcut icon" href="./static/images/jhu_web.png" />

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
    <link rel="stylesheet" href="https://unpkg.com/image-compare-viewer/dist/image-compare-viewer.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://unpkg.com/image-compare-viewer/dist/image-compare-viewer.min.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title"><i>SegFace</i>: Face Segmentation of Long-Tail Classes
                        </h1>
                        <h2 style="font-size: 1.5em; color: dimgray; margin-bottom: .5em; margin-top: -.5em;">AAAI 2025
                        </h2>
                        <div class="is-size-5 publication-authors">
                            <div class="authors-group">
                                <span class="author-block">
                                    <a href="https://kartik-3004.github.io/portfolio/" target="_blank">Kartik
                                        Narayan</a>,
                                </span>
                                <span class="author-block">
                                    <a href="https://vibashan.github.io/" target="_blank">Vibashan VS
                                    </a>,
                                </span>
                                <span class="author-block">
                                    <a href="https://scholar.google.com/citations?user=AkEXTbIAAAAJ&hl=en"
                                        target="_blank">Vishal M. Patel</a>
                                </span>
                            </div>
                        </div>


                        <div class="is-size-5 publication-authors">
                            <span class="author-block">Johns Hopkins University</span>
                        </div>

                        <div class="column has-text-centered">
                            <a href="as"></a>
                            </span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <!-- <span class="link-block">
                                    <a href=""
                                        class="external-link button is-normal is-rounded is-dark" target="_blank">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span> -->
                                <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary material</span>
                </a>
              </span> -->
                                <span class="link-block">
                                    <a href="https://github.com/Kartik-3004/SegFace"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>Arxiv</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://github.com/Kartik-3004/SegFace"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://huggingface.co/kartiknarayan/SegFace"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span>Huggingface Model</span>
                                    </a>
                                </span>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Motivation & Contribution</h2>
                    <img src="./static/images/intro_figure.png" alt="" border="0" height="300" width="750">
                    <div class="content has-text-justified">
                        <p>Figure 1. (a) An illustration of the gallery and probe images from low-resolution dataset
                            (BRIAR).
                            Gallery images usually are high quality compared to the probe images. (b) t-SNE plot for the
                            gallery and probe images of the BRIAR dataset. (c) Average CNN-IQA scores of gallery and
                            probe images for 50 identities of the BRIAR dataset.
                        </p>
                        <img src="./static/images/visual_abstract.png" alt="" border="0" height="300" width="750">
                        <p>Figure 2. The proposed PETAL<i>face</i>: a parameter efficient transfer learning approach
                            adapts to low-resolution datasets
                            beating the performance of pre-trained models with negligible drop in performance on
                            high-resolution and mixed-quality
                            datasets. PETAL<i>face</i> enables development of generalized models achieving competitive
                            performance on
                            high-resolution (LFW, CFP-FP, CPLFW, AgeDB, CALFW, CFP-FF) and mixed-quality datasets
                            (IJB-B, IJB-C) with big
                            enhancements in low-quality surveillance quality datasets (TinyFace, BRIAR, IJB-S)
                        </p>
                        <p>Low-resolution datasets contain images with poor clarity, making it challenging to extract
                            meaningful discriminative features essential for face recognition and verification.
                            Moreover, low-resolution datasets are usually small, with a limited number of subjects, as
                            curating them requires significant time, effort, and investment.
                            Existing methods force the learning of high-resolution and low-resolution face images in a
                            single encoder, failing to account for the domain differences between them. From Figure
                            1(a),
                            1(b), and 1(c), we observe that high-quality gallery images and low-quality probe images
                            belong to distinct domains, and require separate encoders to extract meaningful features for
                            classification. A naive approach to adapting pre-trained models to low-resolution datasets
                            is supervised full fine-tuning on these
                            datasets. However, as mentioned, low-resolution datasets
                            are small in size, and updating a model with a large number
                            of parameters on a small low-resolution dataset results in
                            poor convergence. This makes the model prone to catastrophic forgetting and we see a drop in
                            performance on
                            high-resolution and mixed-quality datasets, as shown in Figure 2. With the above motivation,
                        </p>
                        <ul>
                            <li>
                                We introduce the use of the LoRA-based PETL technique to adapt large pre-trained
                                face-recognition models to low-resolution datasets.
                            </li>
                            <li> We propose an image-quality-based weighting of
                                LoRA modules to create separate proxy encoders for
                                high-resolution and low-resolution data, ensuring effective extraction of embeddings for
                                face recognition.
                            </li>
                            <li> We demonstrate the superiority of PETAL<i>face</i> in
                                adapting to low-resolution datasets, outperforming other state-of-the-art models on
                                low-resolution
                                benchmarks while maintaining performance on high-resolution and mixed-quality datasets.
                            </li>
                        </ul>

                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->

            <!-- Paper video. -->

            <section class="section">
                <div class="container is-max-desktop">
                    <!-- Abstract. -->
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <h2 class="title is-3">PETAL<i>face</i> Framework</h2>
                            <div class="content has-text-justified">
                                <h5 class="subtitle has-text-centered"></h5>

                                <img src="./static/images/petalface.png" alt="" border=0 height=500 width=1500></img>
                                <p>
                                    Overview of the proposed PETAL<i>face</i> approach: We include an additional
                                    trainable module in linear layers present in attention layers and the final feature
                                    projection
                                    MLP. The trainable module is highlighted on the right. Specifically, we add two LoRA
                                    layers, where the weightage
                                    &#x03B1; is decided based on the input-image quality, computed using an
                                    off-the-shelf image quality assessment network
                                    (IQA).

                                </p>


                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section class="section">
                <div class="container is-max-desktop">
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <h2 class="title is-3">Results</h2>
                            <div class="content has-text-justified">
                                <h5 class="subtitle has-text-centered"></h5>
                                <img src="./static/images/tinyface.png" alt="" border="0" height=500 width=1500></img>
                                <p>
                                    Table 1. The models are fine-tuned on train set of TinyFace and tested on several
                                    high-resolution, mixed-quality and TinyFace
                                    dataset. PETAL<i>face</i> adapts to the low-resolution data achieving SOTA
                                    results, preserving its performance on
                                    other datasets. [<span style="color: blue; font-weight: bold;">BLUE</span>]
                                    indicates
                                    the best results for
                                    models trained on
                                    WebFace4M.
                                </p>
                            </div>
                            <div class="content has-text-justified">
                                <h5 class="subtitle has-text-centered"></h5>
                                <img src="./static/images/briar_ijbs.png" alt="" border=0 height=500 width=1500></img>
                                <p>
                                    Table 2. The models are fine-tuned on the BRIAR dataset and tested using BRIAR
                                    Protocol
                                    3.1 and
                                    the IJB-S dataset.
                                    [<span style="color: blue; font-weight: bold;">BLUE</span>] indicates the best
                                    results
                                    for models trained on WebFace4M.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
        </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container content is-max-desktop">
            <h2 class="title">BibTeX</h2>
            <pre>Coming soon !!!</pre>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop content">
            <h5 class="title" style="font-size: 10px;"> Acknowledgement: The website template is taken from
                <span class="author-block">
                    <a href="https://nerfies.github.io/" target="_blank">Nerfies</a>
            </h5>

        </div>
    </section>

    <script>
        const viewers = document.querySelectorAll(".image-compare");
        viewers.forEach((element) => {
            let view = new ImageCompare(element, {
                hoverStart: true,
                addCircle: true
            }).mount();
        });

        $(document).ready(function () {
            var editor = CodeMirror.fromTextArea(document.getElementById("bibtex"), {
                lineNumbers: false,
                lineWrapping: true,
                readOnly: true
            });
            $(function () {
                $('[data-toggle="tooltip"]').tooltip()
            })
        });
    </script>
</body>

</html>